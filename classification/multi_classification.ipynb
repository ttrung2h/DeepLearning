{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_ID</th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run_ID</th>\n",
       "      <th>rerun_ID</th>\n",
       "      <th>cam_col</th>\n",
       "      <th>field_ID</th>\n",
       "      <th>spec_obj_ID</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>MJD</th>\n",
       "      <th>fiber_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.842794</td>\n",
       "      <td>1.678562</td>\n",
       "      <td>-0.189957</td>\n",
       "      <td>0.730051</td>\n",
       "      <td>1.412983</td>\n",
       "      <td>0.900283</td>\n",
       "      <td>0.608321</td>\n",
       "      <td>0.358271</td>\n",
       "      <td>1.842820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.331467</td>\n",
       "      <td>-0.503193</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>1</td>\n",
       "      <td>1.208208</td>\n",
       "      <td>-0.074421</td>\n",
       "      <td>0.109142</td>\n",
       "      <td>1.071997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.675764</td>\n",
       "      <td>-1.582285</td>\n",
       "      <td>-0.653837</td>\n",
       "      <td>-0.181271</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.684916</td>\n",
       "      <td>0.809094</td>\n",
       "      <td>0.849514</td>\n",
       "      <td>1.675851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.960351</td>\n",
       "      <td>1.894562</td>\n",
       "      <td>1.951910</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>1.951948</td>\n",
       "      <td>1.551299</td>\n",
       "      <td>-1.233386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.742588</td>\n",
       "      <td>-1.418968</td>\n",
       "      <td>0.190066</td>\n",
       "      <td>-0.056231</td>\n",
       "      <td>-0.241676</td>\n",
       "      <td>-0.274001</td>\n",
       "      <td>-0.211583</td>\n",
       "      <td>-0.139071</td>\n",
       "      <td>0.742555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297417</td>\n",
       "      <td>-0.421452</td>\n",
       "      <td>-0.940243</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.852440</td>\n",
       "      <td>-0.940230</td>\n",
       "      <td>-0.873920</td>\n",
       "      <td>-0.796538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.870643</td>\n",
       "      <td>1.727888</td>\n",
       "      <td>0.421216</td>\n",
       "      <td>0.587597</td>\n",
       "      <td>-0.186553</td>\n",
       "      <td>-0.140538</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>-0.005492</td>\n",
       "      <td>1.870732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.960351</td>\n",
       "      <td>-0.993643</td>\n",
       "      <td>0.421197</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.852325</td>\n",
       "      <td>0.421189</td>\n",
       "      <td>0.488949</td>\n",
       "      <td>0.458942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.644996</td>\n",
       "      <td>1.803823</td>\n",
       "      <td>-1.075539</td>\n",
       "      <td>1.208900</td>\n",
       "      <td>0.463576</td>\n",
       "      <td>0.698517</td>\n",
       "      <td>0.914775</td>\n",
       "      <td>0.882265</td>\n",
       "      <td>1.644894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.926301</td>\n",
       "      <td>0.682061</td>\n",
       "      <td>-0.323588</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.853049</td>\n",
       "      <td>-0.323567</td>\n",
       "      <td>0.114132</td>\n",
       "      <td>-0.994771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     obj_ID     alpha     delta         u         g         r         i  \\\n",
       "0  1.842794  1.678562 -0.189957  0.730051  1.412983  0.900283  0.608321   \n",
       "1  1.675764 -1.582285 -0.653837 -0.181271  0.330100  0.684916  0.809094   \n",
       "2  0.742588 -1.418968  0.190066 -0.056231 -0.241676 -0.274001 -0.211583   \n",
       "3  1.870643  1.727888  0.421216  0.587597 -0.186553 -0.140538  0.017700   \n",
       "4  1.644996  1.803823 -1.075539  1.208900  0.463576  0.698517  0.914775   \n",
       "\n",
       "          z    run_ID  rerun_ID   cam_col  field_ID  spec_obj_ID  class  \\\n",
       "0  0.358271  1.842820       0.0 -0.331467 -0.503193    -0.074397      1   \n",
       "1  0.849514  1.675851       0.0 -0.960351  1.894562     1.951910      1   \n",
       "2 -0.139071  0.742555       0.0  0.297417 -0.421452    -0.940243      2   \n",
       "3 -0.005492  1.870732       0.0 -0.960351 -0.993643     0.421197      2   \n",
       "4  0.882265  1.644894       0.0  0.926301  0.682061    -0.323588      2   \n",
       "\n",
       "   redshift     plate       MJD  fiber_ID  \n",
       "0  1.208208 -0.074421  0.109142  1.071997  \n",
       "1 -0.004177  1.951948  1.551299 -1.233386  \n",
       "2 -0.852440 -0.940230 -0.873920 -0.796538  \n",
       "3 -0.852325  0.421189  0.488949  0.458942  \n",
       "4 -0.853049 -0.323567  0.114132 -0.994771  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_star.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(labels=['class'],axis=1)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.1566,  1.8858,  0.2564, -2.0829, -0.2722,  1.0747,  1.0373, -0.5046,\n",
       "           0.7159,  0.6177, -0.6187, -0.1081],\n",
       "         [-2.8937,  2.6210, -0.4013,  2.4907,  1.4968, -0.7737, -0.2777,  1.2912,\n",
       "          -0.0525, -0.5826, -0.0252,  0.2450],\n",
       "         [ 1.3644,  0.9952,  0.9485, -0.0967,  0.0874, -0.4827, -1.4682, -0.1757,\n",
       "           1.0385, -0.4812,  0.1117, -0.0340],\n",
       "         [-0.8143,  2.1672, -1.2757, -2.3160,  0.4075, -0.6764,  0.7916,  0.0345,\n",
       "           0.2877, -0.2648,  0.4717,  0.0988],\n",
       "         [-1.2334,  2.1352,  1.4537, -1.8244, -0.3299,  0.6656,  0.0812,  1.9185,\n",
       "          -0.5341, -0.6298,  0.5729, -0.1900]]),\n",
       " tensor([1, 1, 2, 2, 2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to tensor\n",
    "X = torch.tensor(X).to(torch.float32).squeeze()\n",
    "y = torch.tensor(y).to(torch.long)\n",
    "X[:5],y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70277, 12]), torch.Size([70277]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleClassification(nn.Module):\n",
    "    def __init__(self,input_fearures,output_features,hidden_units = 32):\n",
    "        super().__init__()\n",
    "        self.stackLayers = nn.Sequential(\n",
    "            nn.Linear(input_fearures,hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units,output_features)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.stackLayers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat model\n",
    "model = ModuleClassification(input_fearures=12,output_features=3).to(device=device)\n",
    "# Create loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0|Loss: 1.0770|Acc: 47.3179%|Acc test: 47.4894%\n",
      "Epoch 10|Loss: 1.0728|Acc: 48.6411%|Acc test: 48.0283%\n",
      "Epoch 20|Loss: 1.0688|Acc: 48.4775%|Acc test: 47.8327%\n",
      "Epoch 30|Loss: 1.0649|Acc: 47.8088%|Acc test: 47.3257%\n",
      "Epoch 40|Loss: 1.0612|Acc: 47.1756%|Acc test: 47.0660%\n",
      "Epoch 50|Loss: 1.0575|Acc: 46.8981%|Acc test: 46.6747%\n",
      "Epoch 60|Loss: 1.0540|Acc: 46.5637%|Acc test: 46.3421%\n",
      "Epoch 70|Loss: 1.0504|Acc: 46.2009%|Acc test: 46.1144%\n",
      "Epoch 80|Loss: 1.0469|Acc: 46.2223%|Acc test: 46.0451%\n",
      "Epoch 90|Loss: 1.0433|Acc: 46.2151%|Acc test: 46.0415%\n",
      "Epoch 100|Loss: 1.0397|Acc: 46.2578%|Acc test: 46.0860%\n",
      "Epoch 110|Loss: 1.0361|Acc: 46.3574%|Acc test: 46.2087%\n",
      "Epoch 120|Loss: 1.0324|Acc: 46.4713%|Acc test: 46.3652%\n",
      "Epoch 130|Loss: 1.0287|Acc: 46.7558%|Acc test: 46.5929%\n",
      "Epoch 140|Loss: 1.0248|Acc: 47.2040%|Acc test: 46.9149%\n",
      "Epoch 150|Loss: 1.0209|Acc: 47.7163%|Acc test: 47.2866%\n",
      "Epoch 160|Loss: 1.0168|Acc: 48.1431%|Acc test: 47.6459%\n",
      "Epoch 170|Loss: 1.0126|Acc: 48.5913%|Acc test: 48.1475%\n",
      "Epoch 180|Loss: 1.0083|Acc: 48.9755%|Acc test: 48.7167%\n",
      "Epoch 190|Loss: 1.0039|Acc: 49.4664%|Acc test: 49.2396%\n",
      "Epoch 200|Loss: 0.9993|Acc: 50.0142%|Acc test: 49.8497%\n",
      "Epoch 210|Loss: 0.9946|Acc: 50.6190%|Acc test: 50.4082%\n",
      "Epoch 220|Loss: 0.9898|Acc: 51.1027%|Acc test: 51.0254%\n",
      "Epoch 230|Loss: 0.9848|Acc: 51.6505%|Acc test: 51.5839%\n",
      "Epoch 240|Loss: 0.9796|Acc: 52.2837%|Acc test: 52.2741%\n",
      "Epoch 250|Loss: 0.9744|Acc: 52.9027%|Acc test: 52.9144%\n",
      "Epoch 260|Loss: 0.9689|Acc: 53.4078%|Acc test: 53.5885%\n",
      "Epoch 270|Loss: 0.9634|Acc: 54.0979%|Acc test: 54.1826%\n",
      "Epoch 280|Loss: 0.9577|Acc: 54.6742%|Acc test: 54.7571%\n",
      "Epoch 290|Loss: 0.9519|Acc: 55.3571%|Acc test: 55.3227%\n",
      "Epoch 300|Loss: 0.9459|Acc: 55.8694%|Acc test: 55.7567%\n",
      "Epoch 310|Loss: 0.9399|Acc: 56.3389%|Acc test: 56.3277%\n",
      "Epoch 320|Loss: 0.9337|Acc: 56.9294%|Acc test: 56.8969%\n",
      "Epoch 330|Loss: 0.9274|Acc: 57.3919%|Acc test: 57.4269%\n",
      "Epoch 340|Loss: 0.9210|Acc: 57.8258%|Acc test: 57.8734%\n",
      "Epoch 350|Loss: 0.9146|Acc: 58.1673%|Acc test: 58.3839%\n",
      "Epoch 360|Loss: 0.9081|Acc: 58.4590%|Acc test: 58.8214%\n",
      "Epoch 370|Loss: 0.9015|Acc: 58.9428%|Acc test: 59.2341%\n",
      "Epoch 380|Loss: 0.8948|Acc: 59.3270%|Acc test: 59.6592%\n",
      "Epoch 390|Loss: 0.8881|Acc: 59.8036%|Acc test: 60.0647%\n",
      "Epoch 400|Loss: 0.8813|Acc: 60.0953%|Acc test: 60.3315%\n",
      "Epoch 410|Loss: 0.8746|Acc: 60.4297%|Acc test: 60.6072%\n",
      "Epoch 420|Loss: 0.8677|Acc: 60.5507%|Acc test: 60.8509%\n",
      "Epoch 430|Loss: 0.8609|Acc: 60.6431%|Acc test: 61.1729%\n",
      "Epoch 440|Loss: 0.8540|Acc: 60.7712%|Acc test: 61.3721%\n",
      "Epoch 450|Loss: 0.8471|Acc: 61.1340%|Acc test: 61.5962%\n",
      "Epoch 460|Loss: 0.8402|Acc: 61.3332%|Acc test: 61.7883%\n",
      "Epoch 470|Loss: 0.8334|Acc: 61.4755%|Acc test: 61.9199%\n",
      "Epoch 480|Loss: 0.8265|Acc: 61.6961%|Acc test: 62.1351%\n",
      "Epoch 490|Loss: 0.8196|Acc: 61.9806%|Acc test: 62.3183%\n",
      "Epoch 500|Loss: 0.8128|Acc: 62.0945%|Acc test: 62.5016%\n",
      "Epoch 510|Loss: 0.8060|Acc: 62.3435%|Acc test: 62.7097%\n",
      "Epoch 520|Loss: 0.7992|Acc: 62.5000%|Acc test: 62.9107%\n",
      "Epoch 530|Loss: 0.7925|Acc: 62.8273%|Acc test: 63.1561%\n",
      "Epoch 540|Loss: 0.7858|Acc: 63.1190%|Acc test: 63.4087%\n",
      "Epoch 550|Loss: 0.7791|Acc: 63.5102%|Acc test: 63.6524%\n",
      "Epoch 560|Loss: 0.7725|Acc: 63.8090%|Acc test: 63.9387%\n",
      "Epoch 570|Loss: 0.7660|Acc: 63.9798%|Acc test: 64.3194%\n",
      "Epoch 580|Loss: 0.7595|Acc: 64.1577%|Acc test: 64.5684%\n",
      "Epoch 590|Loss: 0.7531|Acc: 64.3711%|Acc test: 64.8886%\n",
      "Epoch 600|Loss: 0.7468|Acc: 64.7339%|Acc test: 65.1696%\n",
      "Epoch 610|Loss: 0.7406|Acc: 65.0825%|Acc test: 65.5574%\n",
      "Epoch 620|Loss: 0.7344|Acc: 65.4169%|Acc test: 65.9184%\n",
      "Epoch 630|Loss: 0.7284|Acc: 65.6944%|Acc test: 66.2528%\n",
      "Epoch 640|Loss: 0.7224|Acc: 65.9861%|Acc test: 66.5979%\n",
      "Epoch 650|Loss: 0.7165|Acc: 66.2564%|Acc test: 66.9181%\n",
      "Epoch 660|Loss: 0.7108|Acc: 66.6335%|Acc test: 67.2364%\n",
      "Epoch 670|Loss: 0.7051|Acc: 66.9038%|Acc test: 67.5299%\n",
      "Epoch 680|Loss: 0.6995|Acc: 67.1955%|Acc test: 67.9177%\n",
      "Epoch 690|Loss: 0.6940|Acc: 67.6722%|Acc test: 68.2734%\n",
      "Epoch 700|Loss: 0.6886|Acc: 68.0065%|Acc test: 68.6683%\n",
      "Epoch 710|Loss: 0.6834|Acc: 68.2769%|Acc test: 69.0347%\n",
      "Epoch 720|Loss: 0.6782|Acc: 68.6682%|Acc test: 69.3638%\n",
      "Epoch 730|Loss: 0.6731|Acc: 69.0808%|Acc test: 69.7480%\n",
      "Epoch 740|Loss: 0.6681|Acc: 69.5646%|Acc test: 70.1108%\n",
      "Epoch 750|Loss: 0.6632|Acc: 69.9772%|Acc test: 70.4826%\n",
      "Epoch 760|Loss: 0.6584|Acc: 70.4326%|Acc test: 70.8863%\n",
      "Epoch 770|Loss: 0.6537|Acc: 70.8167%|Acc test: 71.2225%\n",
      "Epoch 780|Loss: 0.6490|Acc: 71.1582%|Acc test: 71.6547%\n",
      "Epoch 790|Loss: 0.6445|Acc: 71.5139%|Acc test: 72.0407%\n",
      "Epoch 800|Loss: 0.6401|Acc: 71.9977%|Acc test: 72.4018%\n",
      "Epoch 810|Loss: 0.6357|Acc: 72.3179%|Acc test: 72.7682%\n",
      "Epoch 820|Loss: 0.6314|Acc: 72.8372%|Acc test: 73.1684%\n",
      "Epoch 830|Loss: 0.6272|Acc: 73.1858%|Acc test: 73.5473%\n",
      "Epoch 840|Loss: 0.6230|Acc: 73.6269%|Acc test: 73.9154%\n",
      "Epoch 850|Loss: 0.6189|Acc: 74.0182%|Acc test: 74.3672%\n",
      "Epoch 860|Loss: 0.6149|Acc: 74.4308%|Acc test: 74.7230%\n",
      "Epoch 870|Loss: 0.6109|Acc: 74.8293%|Acc test: 75.0983%\n",
      "Epoch 880|Loss: 0.6069|Acc: 75.2703%|Acc test: 75.5180%\n",
      "Epoch 890|Loss: 0.6030|Acc: 75.6688%|Acc test: 75.9200%\n",
      "Epoch 900|Loss: 0.5991|Acc: 76.0885%|Acc test: 76.3380%\n",
      "Epoch 910|Loss: 0.5953|Acc: 76.5581%|Acc test: 76.7400%\n",
      "Epoch 920|Loss: 0.5915|Acc: 76.9849%|Acc test: 77.1438%\n",
      "Epoch 930|Loss: 0.5877|Acc: 77.3122%|Acc test: 77.5458%\n",
      "Epoch 940|Loss: 0.5840|Acc: 77.7035%|Acc test: 77.9282%\n",
      "Epoch 950|Loss: 0.5802|Acc: 78.1090%|Acc test: 78.2946%\n",
      "Epoch 960|Loss: 0.5765|Acc: 78.4647%|Acc test: 78.7624%\n",
      "Epoch 970|Loss: 0.5728|Acc: 78.8204%|Acc test: 79.1519%\n",
      "Epoch 980|Loss: 0.5691|Acc: 79.2971%|Acc test: 79.5610%\n",
      "Epoch 990|Loss: 0.5654|Acc: 79.7311%|Acc test: 79.9790%\n",
      "Epoch 1000|Loss: 0.5618|Acc: 80.1508%|Acc test: 80.3632%\n",
      "Epoch 1010|Loss: 0.5581|Acc: 80.5137%|Acc test: 80.7278%\n",
      "Epoch 1020|Loss: 0.5544|Acc: 80.8053%|Acc test: 81.0694%\n",
      "Epoch 1030|Loss: 0.5508|Acc: 81.2038%|Acc test: 81.3877%\n",
      "Epoch 1040|Loss: 0.5471|Acc: 81.5737%|Acc test: 81.6617%\n",
      "Epoch 1050|Loss: 0.5435|Acc: 81.9365%|Acc test: 81.9854%\n",
      "Epoch 1060|Loss: 0.5398|Acc: 82.2140%|Acc test: 82.3002%\n",
      "Epoch 1070|Loss: 0.5362|Acc: 82.5484%|Acc test: 82.6346%\n",
      "Epoch 1080|Loss: 0.5325|Acc: 82.9112%|Acc test: 82.9210%\n",
      "Epoch 1090|Loss: 0.5289|Acc: 83.2669%|Acc test: 83.2429%\n",
      "Epoch 1100|Loss: 0.5252|Acc: 83.5017%|Acc test: 83.4635%\n",
      "Epoch 1110|Loss: 0.5216|Acc: 83.8361%|Acc test: 83.7374%\n",
      "Epoch 1120|Loss: 0.5179|Acc: 84.1278%|Acc test: 83.9686%\n",
      "Epoch 1130|Loss: 0.5142|Acc: 84.4266%|Acc test: 84.1821%\n",
      "Epoch 1140|Loss: 0.5106|Acc: 84.6400%|Acc test: 84.4097%\n",
      "Epoch 1150|Loss: 0.5069|Acc: 84.8819%|Acc test: 84.6072%\n",
      "Epoch 1160|Loss: 0.5033|Acc: 85.1309%|Acc test: 84.8153%\n",
      "Epoch 1170|Loss: 0.4996|Acc: 85.3443%|Acc test: 85.0341%\n",
      "Epoch 1180|Loss: 0.4960|Acc: 85.5293%|Acc test: 85.2351%\n",
      "Epoch 1190|Loss: 0.4923|Acc: 85.7001%|Acc test: 85.4218%\n",
      "Epoch 1200|Loss: 0.4887|Acc: 85.8495%|Acc test: 85.5944%\n",
      "Epoch 1210|Loss: 0.4851|Acc: 85.9846%|Acc test: 85.7793%\n",
      "Epoch 1220|Loss: 0.4814|Acc: 86.1838%|Acc test: 85.9465%\n",
      "Epoch 1230|Loss: 0.4778|Acc: 86.3403%|Acc test: 86.1084%\n",
      "Epoch 1240|Loss: 0.4742|Acc: 86.4542%|Acc test: 86.2400%\n",
      "Epoch 1250|Loss: 0.4707|Acc: 86.5538%|Acc test: 86.3841%\n",
      "Epoch 1260|Loss: 0.4671|Acc: 86.6890%|Acc test: 86.5477%\n",
      "Epoch 1270|Loss: 0.4636|Acc: 86.8241%|Acc test: 86.6260%\n",
      "Epoch 1280|Loss: 0.4600|Acc: 86.9664%|Acc test: 86.7274%\n",
      "Epoch 1290|Loss: 0.4565|Acc: 87.0447%|Acc test: 86.8323%\n",
      "Epoch 1300|Loss: 0.4531|Acc: 87.2012%|Acc test: 86.9230%\n",
      "Epoch 1310|Loss: 0.4496|Acc: 87.3719%|Acc test: 87.0031%\n",
      "Epoch 1320|Loss: 0.4462|Acc: 87.4787%|Acc test: 87.1151%\n",
      "Epoch 1330|Loss: 0.4429|Acc: 87.5711%|Acc test: 87.2432%\n",
      "Epoch 1340|Loss: 0.4395|Acc: 87.7277%|Acc test: 87.3321%\n",
      "Epoch 1350|Loss: 0.4362|Acc: 87.7846%|Acc test: 87.3997%\n",
      "Epoch 1360|Loss: 0.4330|Acc: 87.8842%|Acc test: 87.4976%\n",
      "Epoch 1370|Loss: 0.4298|Acc: 87.9340%|Acc test: 87.6043%\n",
      "Epoch 1380|Loss: 0.4266|Acc: 87.9624%|Acc test: 87.7057%\n",
      "Epoch 1390|Loss: 0.4234|Acc: 88.0407%|Acc test: 87.7679%\n",
      "Epoch 1400|Loss: 0.4203|Acc: 88.0834%|Acc test: 87.8142%\n",
      "Epoch 1410|Loss: 0.4173|Acc: 88.1616%|Acc test: 87.8764%\n",
      "Epoch 1420|Loss: 0.4142|Acc: 88.1830%|Acc test: 87.9369%\n",
      "Epoch 1430|Loss: 0.4113|Acc: 88.2186%|Acc test: 88.0205%\n",
      "Epoch 1440|Loss: 0.4083|Acc: 88.3039%|Acc test: 88.0738%\n",
      "Epoch 1450|Loss: 0.4054|Acc: 88.3537%|Acc test: 88.1201%\n",
      "Epoch 1460|Loss: 0.4026|Acc: 88.4249%|Acc test: 88.1824%\n",
      "Epoch 1470|Loss: 0.3998|Acc: 88.4604%|Acc test: 88.2748%\n",
      "Epoch 1480|Loss: 0.3970|Acc: 88.5316%|Acc test: 88.3567%\n",
      "Epoch 1490|Loss: 0.3943|Acc: 88.5956%|Acc test: 88.4065%\n",
      "Epoch 1500|Loss: 0.3917|Acc: 88.6810%|Acc test: 88.4403%\n",
      "Epoch 1510|Loss: 0.3890|Acc: 88.7237%|Acc test: 88.5096%\n",
      "Epoch 1520|Loss: 0.3865|Acc: 88.7735%|Acc test: 88.5665%\n",
      "Epoch 1530|Loss: 0.3839|Acc: 88.8090%|Acc test: 88.6341%\n",
      "Epoch 1540|Loss: 0.3814|Acc: 88.8660%|Acc test: 88.6928%\n",
      "Epoch 1550|Loss: 0.3790|Acc: 88.9300%|Acc test: 88.7355%\n",
      "Epoch 1560|Loss: 0.3765|Acc: 88.9940%|Acc test: 88.7764%\n",
      "Epoch 1570|Loss: 0.3742|Acc: 89.0225%|Acc test: 88.8334%\n",
      "Epoch 1580|Loss: 0.3718|Acc: 89.0794%|Acc test: 88.8672%\n",
      "Epoch 1590|Loss: 0.3696|Acc: 89.1007%|Acc test: 88.9134%\n",
      "Epoch 1600|Loss: 0.3673|Acc: 89.1292%|Acc test: 88.9703%\n",
      "Epoch 1610|Loss: 0.3651|Acc: 89.1932%|Acc test: 89.0183%\n",
      "Epoch 1620|Loss: 0.3629|Acc: 89.2217%|Acc test: 89.0664%\n",
      "Epoch 1630|Loss: 0.3608|Acc: 89.2786%|Acc test: 89.1144%\n",
      "Epoch 1640|Loss: 0.3587|Acc: 89.3497%|Acc test: 89.1589%\n",
      "Epoch 1650|Loss: 0.3566|Acc: 89.3711%|Acc test: 89.2122%\n",
      "Epoch 1660|Loss: 0.3546|Acc: 89.3924%|Acc test: 89.2513%\n",
      "Epoch 1670|Loss: 0.3526|Acc: 89.4778%|Acc test: 89.3083%\n",
      "Epoch 1680|Loss: 0.3506|Acc: 89.5134%|Acc test: 89.3421%\n",
      "Epoch 1690|Loss: 0.3487|Acc: 89.5418%|Acc test: 89.4079%\n",
      "Epoch 1700|Loss: 0.3468|Acc: 89.5916%|Acc test: 89.4257%\n",
      "Epoch 1710|Loss: 0.3449|Acc: 89.6628%|Acc test: 89.4719%\n",
      "Epoch 1720|Loss: 0.3431|Acc: 89.6770%|Acc test: 89.5146%\n",
      "Epoch 1730|Loss: 0.3413|Acc: 89.7055%|Acc test: 89.5644%\n",
      "Epoch 1740|Loss: 0.3395|Acc: 89.7624%|Acc test: 89.6017%\n",
      "Epoch 1750|Loss: 0.3378|Acc: 89.8477%|Acc test: 89.6427%\n",
      "Epoch 1760|Loss: 0.3360|Acc: 89.8976%|Acc test: 89.6782%\n",
      "Epoch 1770|Loss: 0.3343|Acc: 89.9189%|Acc test: 89.7156%\n",
      "Epoch 1780|Loss: 0.3326|Acc: 89.9331%|Acc test: 89.7583%\n",
      "Epoch 1790|Loss: 0.3310|Acc: 89.9972%|Acc test: 89.8099%\n",
      "Epoch 1800|Loss: 0.3293|Acc: 90.0683%|Acc test: 89.8614%\n",
      "Epoch 1810|Loss: 0.3277|Acc: 90.1537%|Acc test: 89.9077%\n",
      "Epoch 1820|Loss: 0.3261|Acc: 90.2319%|Acc test: 89.9628%\n",
      "Epoch 1830|Loss: 0.3245|Acc: 90.2675%|Acc test: 90.0020%\n",
      "Epoch 1840|Loss: 0.3229|Acc: 90.2888%|Acc test: 90.0535%\n",
      "Epoch 1850|Loss: 0.3213|Acc: 90.3458%|Acc test: 90.0998%\n",
      "Epoch 1860|Loss: 0.3198|Acc: 90.3884%|Acc test: 90.1496%\n",
      "Epoch 1870|Loss: 0.3182|Acc: 90.4098%|Acc test: 90.1852%\n",
      "Epoch 1880|Loss: 0.3167|Acc: 90.4596%|Acc test: 90.2225%\n",
      "Epoch 1890|Loss: 0.3152|Acc: 90.5307%|Acc test: 90.2545%\n",
      "Epoch 1900|Loss: 0.3136|Acc: 90.5734%|Acc test: 90.3026%\n",
      "Epoch 1910|Loss: 0.3121|Acc: 90.5805%|Acc test: 90.3399%\n",
      "Epoch 1920|Loss: 0.3105|Acc: 90.6090%|Acc test: 90.3808%\n",
      "Epoch 1930|Loss: 0.3090|Acc: 90.6517%|Acc test: 90.4253%\n",
      "Epoch 1940|Loss: 0.3075|Acc: 90.6873%|Acc test: 90.4698%\n",
      "Epoch 1950|Loss: 0.3060|Acc: 90.7940%|Acc test: 90.5320%\n",
      "Epoch 1960|Loss: 0.3044|Acc: 90.8367%|Acc test: 90.5996%\n",
      "Epoch 1970|Loss: 0.3028|Acc: 90.9007%|Acc test: 90.6725%\n",
      "Epoch 1980|Loss: 0.3013|Acc: 90.9363%|Acc test: 90.7419%\n",
      "Epoch 1990|Loss: 0.2997|Acc: 91.0287%|Acc test: 90.7810%\n",
      "Epoch 2000|Loss: 0.2981|Acc: 91.1212%|Acc test: 90.8166%\n",
      "Epoch 2010|Loss: 0.2966|Acc: 91.1568%|Acc test: 90.8877%\n",
      "Epoch 2020|Loss: 0.2950|Acc: 91.1924%|Acc test: 90.9500%\n",
      "Epoch 2030|Loss: 0.2934|Acc: 91.2777%|Acc test: 91.0069%\n",
      "Epoch 2040|Loss: 0.2919|Acc: 91.3133%|Acc test: 91.0674%\n",
      "Epoch 2050|Loss: 0.2904|Acc: 91.3702%|Acc test: 91.1047%\n",
      "Epoch 2060|Loss: 0.2889|Acc: 91.4414%|Acc test: 91.1670%\n",
      "Epoch 2070|Loss: 0.2875|Acc: 91.5339%|Acc test: 91.2364%\n",
      "Epoch 2080|Loss: 0.2861|Acc: 91.5410%|Acc test: 91.2826%\n",
      "Epoch 2090|Loss: 0.2847|Acc: 91.6264%|Acc test: 91.3217%\n",
      "Epoch 2100|Loss: 0.2834|Acc: 91.6690%|Acc test: 91.3555%\n",
      "Epoch 2110|Loss: 0.2822|Acc: 91.6690%|Acc test: 91.3733%\n",
      "Epoch 2120|Loss: 0.2810|Acc: 91.7758%|Acc test: 91.4356%\n",
      "Epoch 2130|Loss: 0.2798|Acc: 91.8540%|Acc test: 91.4836%\n",
      "Epoch 2140|Loss: 0.2786|Acc: 91.9252%|Acc test: 91.5423%\n",
      "Epoch 2150|Loss: 0.2775|Acc: 91.9750%|Acc test: 91.5992%\n",
      "Epoch 2160|Loss: 0.2764|Acc: 91.9892%|Acc test: 91.6490%\n",
      "Epoch 2170|Loss: 0.2754|Acc: 92.0603%|Acc test: 91.6971%\n",
      "Epoch 2180|Loss: 0.2743|Acc: 92.0888%|Acc test: 91.7646%\n",
      "Epoch 2190|Loss: 0.2733|Acc: 92.1101%|Acc test: 91.7931%\n",
      "Epoch 2200|Loss: 0.2723|Acc: 92.1315%|Acc test: 91.8251%\n",
      "Epoch 2210|Loss: 0.2713|Acc: 92.1742%|Acc test: 91.8625%\n",
      "Epoch 2220|Loss: 0.2704|Acc: 92.2168%|Acc test: 91.8909%\n",
      "Epoch 2230|Loss: 0.2694|Acc: 92.2240%|Acc test: 91.9265%\n",
      "Epoch 2240|Loss: 0.2685|Acc: 92.2311%|Acc test: 91.9514%\n",
      "Epoch 2250|Loss: 0.2676|Acc: 92.2595%|Acc test: 91.9799%\n",
      "Epoch 2260|Loss: 0.2667|Acc: 92.2666%|Acc test: 92.0048%\n",
      "Epoch 2270|Loss: 0.2658|Acc: 92.2809%|Acc test: 92.0243%\n",
      "Epoch 2280|Loss: 0.2649|Acc: 92.2951%|Acc test: 92.0510%\n",
      "Epoch 2290|Loss: 0.2641|Acc: 92.3022%|Acc test: 92.0866%\n",
      "Epoch 2300|Loss: 0.2632|Acc: 92.3093%|Acc test: 92.1222%\n",
      "Epoch 2310|Loss: 0.2624|Acc: 92.3449%|Acc test: 92.1453%\n",
      "Epoch 2320|Loss: 0.2616|Acc: 92.3520%|Acc test: 92.1773%\n",
      "Epoch 2330|Loss: 0.2608|Acc: 92.3734%|Acc test: 92.2058%\n",
      "Epoch 2340|Loss: 0.2600|Acc: 92.3876%|Acc test: 92.2235%\n",
      "Epoch 2350|Loss: 0.2592|Acc: 92.4160%|Acc test: 92.2413%\n",
      "Epoch 2360|Loss: 0.2584|Acc: 92.4374%|Acc test: 92.2733%\n",
      "Epoch 2370|Loss: 0.2577|Acc: 92.4659%|Acc test: 92.2983%\n",
      "Epoch 2380|Loss: 0.2569|Acc: 92.5370%|Acc test: 92.3249%\n",
      "Epoch 2390|Loss: 0.2561|Acc: 92.5299%|Acc test: 92.3534%\n",
      "Epoch 2400|Loss: 0.2554|Acc: 92.5370%|Acc test: 92.3819%\n",
      "Epoch 2410|Loss: 0.2547|Acc: 92.5512%|Acc test: 92.4032%\n",
      "Epoch 2420|Loss: 0.2539|Acc: 92.5797%|Acc test: 92.4245%\n",
      "Epoch 2430|Loss: 0.2532|Acc: 92.5868%|Acc test: 92.4583%\n",
      "Epoch 2440|Loss: 0.2525|Acc: 92.6081%|Acc test: 92.4797%\n",
      "Epoch 2450|Loss: 0.2518|Acc: 92.6153%|Acc test: 92.5010%\n",
      "Epoch 2460|Loss: 0.2511|Acc: 92.6224%|Acc test: 92.5188%\n",
      "Epoch 2470|Loss: 0.2505|Acc: 92.6437%|Acc test: 92.5455%\n",
      "Epoch 2480|Loss: 0.2498|Acc: 92.6651%|Acc test: 92.5526%\n",
      "Epoch 2490|Loss: 0.2491|Acc: 92.6722%|Acc test: 92.5811%\n",
      "Epoch 2500|Loss: 0.2485|Acc: 92.6935%|Acc test: 92.6006%\n",
      "Epoch 2510|Loss: 0.2478|Acc: 92.7149%|Acc test: 92.6184%\n",
      "Epoch 2520|Loss: 0.2472|Acc: 92.7149%|Acc test: 92.6362%\n",
      "Epoch 2530|Loss: 0.2465|Acc: 92.7575%|Acc test: 92.6504%\n",
      "Epoch 2540|Loss: 0.2459|Acc: 92.7504%|Acc test: 92.6575%\n",
      "Epoch 2550|Loss: 0.2453|Acc: 92.7504%|Acc test: 92.6718%\n",
      "Epoch 2560|Loss: 0.2447|Acc: 92.7647%|Acc test: 92.6842%\n",
      "Epoch 2570|Loss: 0.2441|Acc: 92.7860%|Acc test: 92.7020%\n",
      "Epoch 2580|Loss: 0.2435|Acc: 92.8073%|Acc test: 92.7180%\n",
      "Epoch 2590|Loss: 0.2429|Acc: 92.8216%|Acc test: 92.7447%\n",
      "Epoch 2600|Loss: 0.2423|Acc: 92.8358%|Acc test: 92.7607%\n",
      "Epoch 2610|Loss: 0.2417|Acc: 92.8358%|Acc test: 92.7821%\n",
      "Epoch 2620|Loss: 0.2411|Acc: 92.8643%|Acc test: 92.7963%\n",
      "Epoch 2630|Loss: 0.2405|Acc: 92.8785%|Acc test: 92.8087%\n",
      "Epoch 2640|Loss: 0.2400|Acc: 92.8785%|Acc test: 92.8319%\n",
      "Epoch 2650|Loss: 0.2394|Acc: 92.8927%|Acc test: 92.8514%\n",
      "Epoch 2660|Loss: 0.2389|Acc: 92.8998%|Acc test: 92.8621%\n",
      "Epoch 2670|Loss: 0.2383|Acc: 92.9212%|Acc test: 92.8852%\n",
      "Epoch 2680|Loss: 0.2378|Acc: 92.9496%|Acc test: 92.8977%\n",
      "Epoch 2690|Loss: 0.2372|Acc: 92.9710%|Acc test: 92.9119%\n",
      "Epoch 2700|Loss: 0.2367|Acc: 92.9781%|Acc test: 92.9332%\n",
      "Epoch 2710|Loss: 0.2362|Acc: 93.0137%|Acc test: 92.9475%\n",
      "Epoch 2720|Loss: 0.2356|Acc: 93.0492%|Acc test: 92.9635%\n",
      "Epoch 2730|Loss: 0.2351|Acc: 93.0492%|Acc test: 92.9795%\n",
      "Epoch 2740|Loss: 0.2346|Acc: 93.0848%|Acc test: 92.9919%\n",
      "Epoch 2750|Loss: 0.2341|Acc: 93.1133%|Acc test: 93.0044%\n",
      "Epoch 2760|Loss: 0.2336|Acc: 93.1133%|Acc test: 93.0115%\n",
      "Epoch 2770|Loss: 0.2331|Acc: 93.1275%|Acc test: 93.0311%\n",
      "Epoch 2780|Loss: 0.2326|Acc: 93.1488%|Acc test: 93.0471%\n",
      "Epoch 2790|Loss: 0.2321|Acc: 93.1488%|Acc test: 93.0720%\n",
      "Epoch 2800|Loss: 0.2316|Acc: 93.1773%|Acc test: 93.0951%\n",
      "Epoch 2810|Loss: 0.2311|Acc: 93.1773%|Acc test: 93.1129%\n",
      "Epoch 2820|Loss: 0.2306|Acc: 93.1773%|Acc test: 93.1271%\n",
      "Epoch 2830|Loss: 0.2302|Acc: 93.1986%|Acc test: 93.1325%\n",
      "Epoch 2840|Loss: 0.2297|Acc: 93.2271%|Acc test: 93.1378%\n",
      "Epoch 2850|Loss: 0.2292|Acc: 93.2271%|Acc test: 93.1467%\n",
      "Epoch 2860|Loss: 0.2288|Acc: 93.2484%|Acc test: 93.1609%\n",
      "Epoch 2870|Loss: 0.2283|Acc: 93.2698%|Acc test: 93.1823%\n",
      "Epoch 2880|Loss: 0.2278|Acc: 93.2698%|Acc test: 93.1912%\n",
      "Epoch 2890|Loss: 0.2274|Acc: 93.2627%|Acc test: 93.2054%\n",
      "Epoch 2900|Loss: 0.2269|Acc: 93.2840%|Acc test: 93.2196%\n",
      "Epoch 2910|Loss: 0.2265|Acc: 93.2982%|Acc test: 93.2410%\n",
      "Epoch 2920|Loss: 0.2260|Acc: 93.3196%|Acc test: 93.2552%\n",
      "Epoch 2930|Loss: 0.2256|Acc: 93.3338%|Acc test: 93.2730%\n",
      "Epoch 2940|Loss: 0.2251|Acc: 93.3551%|Acc test: 93.2925%\n",
      "Epoch 2950|Loss: 0.2247|Acc: 93.3551%|Acc test: 93.3068%\n",
      "Epoch 2960|Loss: 0.2243|Acc: 93.3694%|Acc test: 93.3157%\n",
      "Epoch 2970|Loss: 0.2238|Acc: 93.3836%|Acc test: 93.3317%\n",
      "Epoch 2980|Loss: 0.2234|Acc: 93.4121%|Acc test: 93.3459%\n",
      "Epoch 2990|Loss: 0.2230|Acc: 93.4192%|Acc test: 93.3566%\n",
      "Epoch 3000|Loss: 0.2226|Acc: 93.4263%|Acc test: 93.3744%\n",
      "Epoch 3010|Loss: 0.2222|Acc: 93.4405%|Acc test: 93.3815%\n",
      "Epoch 3020|Loss: 0.2218|Acc: 93.4334%|Acc test: 93.3939%\n",
      "Epoch 3030|Loss: 0.2213|Acc: 93.4405%|Acc test: 93.4099%\n",
      "Epoch 3040|Loss: 0.2209|Acc: 93.4476%|Acc test: 93.4224%\n",
      "Epoch 3050|Loss: 0.2205|Acc: 93.4690%|Acc test: 93.4313%\n",
      "Epoch 3060|Loss: 0.2201|Acc: 93.4832%|Acc test: 93.4473%\n",
      "Epoch 3070|Loss: 0.2197|Acc: 93.5117%|Acc test: 93.4615%\n",
      "Epoch 3080|Loss: 0.2193|Acc: 93.5259%|Acc test: 93.4757%\n",
      "Epoch 3090|Loss: 0.2190|Acc: 93.5472%|Acc test: 93.4846%\n",
      "Epoch 3100|Loss: 0.2186|Acc: 93.5615%|Acc test: 93.4989%\n",
      "Epoch 3110|Loss: 0.2182|Acc: 93.5686%|Acc test: 93.5042%\n",
      "Epoch 3120|Loss: 0.2178|Acc: 93.5899%|Acc test: 93.5184%\n",
      "Epoch 3130|Loss: 0.2174|Acc: 93.5899%|Acc test: 93.5273%\n",
      "Epoch 3140|Loss: 0.2170|Acc: 93.5828%|Acc test: 93.5273%\n",
      "Epoch 3150|Loss: 0.2167|Acc: 93.5899%|Acc test: 93.5416%\n",
      "Epoch 3160|Loss: 0.2163|Acc: 93.6184%|Acc test: 93.5629%\n",
      "Epoch 3170|Loss: 0.2159|Acc: 93.6397%|Acc test: 93.5754%\n",
      "Epoch 3180|Loss: 0.2155|Acc: 93.6682%|Acc test: 93.5771%\n",
      "Epoch 3190|Loss: 0.2152|Acc: 93.6895%|Acc test: 93.5878%\n",
      "Epoch 3200|Loss: 0.2148|Acc: 93.6895%|Acc test: 93.5949%\n",
      "Epoch 3210|Loss: 0.2145|Acc: 93.7180%|Acc test: 93.6038%\n",
      "Epoch 3220|Loss: 0.2141|Acc: 93.7180%|Acc test: 93.6109%\n",
      "Epoch 3230|Loss: 0.2138|Acc: 93.7322%|Acc test: 93.6180%\n",
      "Epoch 3240|Loss: 0.2134|Acc: 93.7464%|Acc test: 93.6287%\n",
      "Epoch 3250|Loss: 0.2130|Acc: 93.7607%|Acc test: 93.6341%\n",
      "Epoch 3260|Loss: 0.2127|Acc: 93.7607%|Acc test: 93.6572%\n",
      "Epoch 3270|Loss: 0.2124|Acc: 93.7678%|Acc test: 93.6625%\n",
      "Epoch 3280|Loss: 0.2120|Acc: 93.7749%|Acc test: 93.6714%\n",
      "Epoch 3290|Loss: 0.2117|Acc: 93.7891%|Acc test: 93.6767%\n",
      "Epoch 3300|Loss: 0.2113|Acc: 93.8105%|Acc test: 93.6892%\n",
      "Epoch 3310|Loss: 0.2110|Acc: 93.8176%|Acc test: 93.7016%\n",
      "Epoch 3320|Loss: 0.2107|Acc: 93.8318%|Acc test: 93.7088%\n",
      "Epoch 3330|Loss: 0.2103|Acc: 93.8460%|Acc test: 93.7212%\n",
      "Epoch 3340|Loss: 0.2100|Acc: 93.8460%|Acc test: 93.7230%\n",
      "Epoch 3350|Loss: 0.2097|Acc: 93.8532%|Acc test: 93.7354%\n",
      "Epoch 3360|Loss: 0.2094|Acc: 93.8532%|Acc test: 93.7532%\n",
      "Epoch 3370|Loss: 0.2090|Acc: 93.8532%|Acc test: 93.7568%\n",
      "Epoch 3380|Loss: 0.2087|Acc: 93.8532%|Acc test: 93.7621%\n",
      "Epoch 3390|Loss: 0.2084|Acc: 93.8460%|Acc test: 93.7675%\n",
      "Epoch 3400|Loss: 0.2081|Acc: 93.8603%|Acc test: 93.7852%\n",
      "Epoch 3410|Loss: 0.2078|Acc: 93.8532%|Acc test: 93.8012%\n",
      "Epoch 3420|Loss: 0.2075|Acc: 93.8603%|Acc test: 93.8101%\n",
      "Epoch 3430|Loss: 0.2072|Acc: 93.8816%|Acc test: 93.8244%\n",
      "Epoch 3440|Loss: 0.2068|Acc: 93.8887%|Acc test: 93.8297%\n",
      "Epoch 3450|Loss: 0.2065|Acc: 93.8958%|Acc test: 93.8368%\n",
      "Epoch 3460|Loss: 0.2062|Acc: 93.9101%|Acc test: 93.8493%\n",
      "Epoch 3470|Loss: 0.2059|Acc: 93.9243%|Acc test: 93.8599%\n",
      "Epoch 3480|Loss: 0.2056|Acc: 93.9385%|Acc test: 93.8671%\n",
      "Epoch 3490|Loss: 0.2053|Acc: 93.9456%|Acc test: 93.8742%\n",
      "Epoch 3500|Loss: 0.2051|Acc: 93.9528%|Acc test: 93.8795%\n",
      "Epoch 3510|Loss: 0.2048|Acc: 93.9528%|Acc test: 93.8902%\n",
      "Epoch 3520|Loss: 0.2045|Acc: 93.9528%|Acc test: 93.9026%\n",
      "Epoch 3530|Loss: 0.2042|Acc: 93.9741%|Acc test: 93.9097%\n",
      "Epoch 3540|Loss: 0.2039|Acc: 93.9812%|Acc test: 93.9258%\n",
      "Epoch 3550|Loss: 0.2036|Acc: 93.9812%|Acc test: 93.9293%\n",
      "Epoch 3560|Loss: 0.2033|Acc: 93.9883%|Acc test: 93.9329%\n",
      "Epoch 3570|Loss: 0.2031|Acc: 94.0097%|Acc test: 93.9382%\n",
      "Epoch 3580|Loss: 0.2028|Acc: 94.0168%|Acc test: 93.9489%\n",
      "Epoch 3590|Loss: 0.2025|Acc: 94.0168%|Acc test: 93.9596%\n",
      "Epoch 3600|Loss: 0.2022|Acc: 94.0239%|Acc test: 93.9613%\n",
      "Epoch 3610|Loss: 0.2019|Acc: 94.0524%|Acc test: 93.9738%\n",
      "Epoch 3620|Loss: 0.2017|Acc: 94.0595%|Acc test: 93.9827%\n",
      "Epoch 3630|Loss: 0.2014|Acc: 94.0666%|Acc test: 93.9898%\n",
      "Epoch 3640|Loss: 0.2011|Acc: 94.0808%|Acc test: 93.9969%\n",
      "Epoch 3650|Loss: 0.2009|Acc: 94.0808%|Acc test: 94.0040%\n",
      "Epoch 3660|Loss: 0.2006|Acc: 94.0808%|Acc test: 94.0147%\n",
      "Epoch 3670|Loss: 0.2003|Acc: 94.0951%|Acc test: 94.0218%\n",
      "Epoch 3680|Loss: 0.2001|Acc: 94.0951%|Acc test: 94.0307%\n",
      "Epoch 3690|Loss: 0.1998|Acc: 94.0879%|Acc test: 94.0538%\n",
      "Epoch 3700|Loss: 0.1996|Acc: 94.0879%|Acc test: 94.0609%\n",
      "Epoch 3710|Loss: 0.1993|Acc: 94.0951%|Acc test: 94.0769%\n",
      "Epoch 3720|Loss: 0.1990|Acc: 94.1235%|Acc test: 94.0841%\n",
      "Epoch 3730|Loss: 0.1988|Acc: 94.1306%|Acc test: 94.0894%\n",
      "Epoch 3740|Loss: 0.1985|Acc: 94.1377%|Acc test: 94.1018%\n",
      "Epoch 3750|Loss: 0.1983|Acc: 94.1377%|Acc test: 94.1090%\n",
      "Epoch 3760|Loss: 0.1980|Acc: 94.1520%|Acc test: 94.1196%\n",
      "Epoch 3770|Loss: 0.1978|Acc: 94.1520%|Acc test: 94.1267%\n",
      "Epoch 3780|Loss: 0.1975|Acc: 94.1733%|Acc test: 94.1267%\n",
      "Epoch 3790|Loss: 0.1973|Acc: 94.1875%|Acc test: 94.1339%\n",
      "Epoch 3800|Loss: 0.1970|Acc: 94.2018%|Acc test: 94.1374%\n",
      "Epoch 3810|Loss: 0.1968|Acc: 94.2018%|Acc test: 94.1410%\n",
      "Epoch 3820|Loss: 0.1966|Acc: 94.2018%|Acc test: 94.1445%\n",
      "Epoch 3830|Loss: 0.1963|Acc: 94.2089%|Acc test: 94.1499%\n",
      "Epoch 3840|Loss: 0.1961|Acc: 94.2231%|Acc test: 94.1570%\n",
      "Epoch 3850|Loss: 0.1958|Acc: 94.2302%|Acc test: 94.1605%\n",
      "Epoch 3860|Loss: 0.1956|Acc: 94.2373%|Acc test: 94.1641%\n",
      "Epoch 3870|Loss: 0.1954|Acc: 94.2445%|Acc test: 94.1694%\n",
      "Epoch 3880|Loss: 0.1951|Acc: 94.2302%|Acc test: 94.1748%\n",
      "Epoch 3890|Loss: 0.1949|Acc: 94.2302%|Acc test: 94.1926%\n",
      "Epoch 3900|Loss: 0.1947|Acc: 94.2302%|Acc test: 94.2068%\n",
      "Epoch 3910|Loss: 0.1944|Acc: 94.2516%|Acc test: 94.2104%\n",
      "Epoch 3920|Loss: 0.1942|Acc: 94.2516%|Acc test: 94.2228%\n",
      "Epoch 3930|Loss: 0.1940|Acc: 94.2587%|Acc test: 94.2281%\n",
      "Epoch 3940|Loss: 0.1938|Acc: 94.2658%|Acc test: 94.2264%\n",
      "Epoch 3950|Loss: 0.1935|Acc: 94.2729%|Acc test: 94.2335%\n",
      "Epoch 3960|Loss: 0.1933|Acc: 94.2871%|Acc test: 94.2388%\n",
      "Epoch 3970|Loss: 0.1931|Acc: 94.2871%|Acc test: 94.2441%\n",
      "Epoch 3980|Loss: 0.1929|Acc: 94.3085%|Acc test: 94.2513%\n",
      "Epoch 3990|Loss: 0.1927|Acc: 94.3085%|Acc test: 94.2619%\n",
      "Epoch 4000|Loss: 0.1924|Acc: 94.3085%|Acc test: 94.2673%\n",
      "Epoch 4010|Loss: 0.1922|Acc: 94.3085%|Acc test: 94.2708%\n",
      "Epoch 4020|Loss: 0.1920|Acc: 94.3298%|Acc test: 94.2744%\n",
      "Epoch 4030|Loss: 0.1918|Acc: 94.3298%|Acc test: 94.2762%\n",
      "Epoch 4040|Loss: 0.1916|Acc: 94.3654%|Acc test: 94.2779%\n",
      "Epoch 4050|Loss: 0.1914|Acc: 94.3725%|Acc test: 94.2833%\n",
      "Epoch 4060|Loss: 0.1912|Acc: 94.3796%|Acc test: 94.2939%\n",
      "Epoch 4070|Loss: 0.1909|Acc: 94.3939%|Acc test: 94.3028%\n",
      "Epoch 4080|Loss: 0.1907|Acc: 94.4010%|Acc test: 94.3100%\n",
      "Epoch 4090|Loss: 0.1905|Acc: 94.4010%|Acc test: 94.3206%\n",
      "Epoch 4100|Loss: 0.1903|Acc: 94.4010%|Acc test: 94.3277%\n",
      "Epoch 4110|Loss: 0.1901|Acc: 94.4010%|Acc test: 94.3295%\n",
      "Epoch 4120|Loss: 0.1899|Acc: 94.4010%|Acc test: 94.3384%\n",
      "Epoch 4130|Loss: 0.1897|Acc: 94.4152%|Acc test: 94.3455%\n",
      "Epoch 4140|Loss: 0.1895|Acc: 94.4223%|Acc test: 94.3491%\n",
      "Epoch 4150|Loss: 0.1893|Acc: 94.4223%|Acc test: 94.3598%\n",
      "Epoch 4160|Loss: 0.1891|Acc: 94.4294%|Acc test: 94.3722%\n",
      "Epoch 4170|Loss: 0.1889|Acc: 94.4365%|Acc test: 94.3811%\n",
      "Epoch 4180|Loss: 0.1887|Acc: 94.4365%|Acc test: 94.3829%\n",
      "Epoch 4190|Loss: 0.1885|Acc: 94.4437%|Acc test: 94.3864%\n",
      "Epoch 4200|Loss: 0.1883|Acc: 94.4579%|Acc test: 94.3864%\n",
      "Epoch 4210|Loss: 0.1881|Acc: 94.4650%|Acc test: 94.3918%\n",
      "Epoch 4220|Loss: 0.1879|Acc: 94.4650%|Acc test: 94.3953%\n",
      "Epoch 4230|Loss: 0.1878|Acc: 94.4650%|Acc test: 94.4007%\n",
      "Epoch 4240|Loss: 0.1876|Acc: 94.4650%|Acc test: 94.4078%\n",
      "Epoch 4250|Loss: 0.1874|Acc: 94.4721%|Acc test: 94.4131%\n",
      "Epoch 4260|Loss: 0.1872|Acc: 94.4721%|Acc test: 94.4220%\n",
      "Epoch 4270|Loss: 0.1870|Acc: 94.4792%|Acc test: 94.4238%\n",
      "Epoch 4280|Loss: 0.1868|Acc: 94.4792%|Acc test: 94.4256%\n",
      "Epoch 4290|Loss: 0.1866|Acc: 94.4792%|Acc test: 94.4309%\n",
      "Epoch 4300|Loss: 0.1864|Acc: 94.4792%|Acc test: 94.4416%\n",
      "Epoch 4310|Loss: 0.1863|Acc: 94.4792%|Acc test: 94.4434%\n",
      "Epoch 4320|Loss: 0.1861|Acc: 94.4863%|Acc test: 94.4523%\n",
      "Epoch 4330|Loss: 0.1859|Acc: 94.4863%|Acc test: 94.4594%\n",
      "Epoch 4340|Loss: 0.1857|Acc: 94.5006%|Acc test: 94.4629%\n",
      "Epoch 4350|Loss: 0.1855|Acc: 94.4935%|Acc test: 94.4629%\n",
      "Epoch 4360|Loss: 0.1853|Acc: 94.4935%|Acc test: 94.4683%\n",
      "Epoch 4370|Loss: 0.1852|Acc: 94.5006%|Acc test: 94.4700%\n",
      "Epoch 4380|Loss: 0.1850|Acc: 94.5077%|Acc test: 94.4736%\n",
      "Epoch 4390|Loss: 0.1848|Acc: 94.5219%|Acc test: 94.4825%\n",
      "Epoch 4400|Loss: 0.1846|Acc: 94.5219%|Acc test: 94.4878%\n",
      "Epoch 4410|Loss: 0.1845|Acc: 94.5290%|Acc test: 94.4896%\n",
      "Epoch 4420|Loss: 0.1843|Acc: 94.5290%|Acc test: 94.4967%\n",
      "Epoch 4430|Loss: 0.1841|Acc: 94.5361%|Acc test: 94.5074%\n",
      "Epoch 4440|Loss: 0.1839|Acc: 94.5361%|Acc test: 94.5109%\n",
      "Epoch 4450|Loss: 0.1838|Acc: 94.5361%|Acc test: 94.5145%\n",
      "Epoch 4460|Loss: 0.1836|Acc: 94.5433%|Acc test: 94.5198%\n",
      "Epoch 4470|Loss: 0.1834|Acc: 94.5361%|Acc test: 94.5270%\n",
      "Epoch 4480|Loss: 0.1833|Acc: 94.5361%|Acc test: 94.5234%\n",
      "Epoch 4490|Loss: 0.1831|Acc: 94.5361%|Acc test: 94.5287%\n",
      "Epoch 4500|Loss: 0.1829|Acc: 94.5575%|Acc test: 94.5359%\n",
      "Epoch 4510|Loss: 0.1828|Acc: 94.5646%|Acc test: 94.5376%\n",
      "Epoch 4520|Loss: 0.1826|Acc: 94.5717%|Acc test: 94.5412%\n",
      "Epoch 4530|Loss: 0.1824|Acc: 94.5717%|Acc test: 94.5501%\n",
      "Epoch 4540|Loss: 0.1823|Acc: 94.5717%|Acc test: 94.5554%\n",
      "Epoch 4550|Loss: 0.1821|Acc: 94.5788%|Acc test: 94.5608%\n",
      "Epoch 4560|Loss: 0.1819|Acc: 94.6073%|Acc test: 94.5679%\n",
      "Epoch 4570|Loss: 0.1818|Acc: 94.6073%|Acc test: 94.5679%\n",
      "Epoch 4580|Loss: 0.1816|Acc: 94.6144%|Acc test: 94.5750%\n",
      "Epoch 4590|Loss: 0.1814|Acc: 94.6073%|Acc test: 94.5839%\n",
      "Epoch 4600|Loss: 0.1813|Acc: 94.6144%|Acc test: 94.5874%\n",
      "Epoch 4610|Loss: 0.1811|Acc: 94.6215%|Acc test: 94.5945%\n",
      "Epoch 4620|Loss: 0.1810|Acc: 94.6357%|Acc test: 94.5981%\n",
      "Epoch 4630|Loss: 0.1808|Acc: 94.6357%|Acc test: 94.6052%\n",
      "Epoch 4640|Loss: 0.1806|Acc: 94.6500%|Acc test: 94.6106%\n",
      "Epoch 4650|Loss: 0.1805|Acc: 94.6642%|Acc test: 94.6177%\n",
      "Epoch 4660|Loss: 0.1803|Acc: 94.6642%|Acc test: 94.6230%\n",
      "Epoch 4670|Loss: 0.1802|Acc: 94.6784%|Acc test: 94.6212%\n",
      "Epoch 4680|Loss: 0.1800|Acc: 94.6927%|Acc test: 94.6283%\n",
      "Epoch 4690|Loss: 0.1799|Acc: 94.7069%|Acc test: 94.6390%\n",
      "Epoch 4700|Loss: 0.1797|Acc: 94.7069%|Acc test: 94.6479%\n",
      "Epoch 4710|Loss: 0.1795|Acc: 94.7069%|Acc test: 94.6604%\n",
      "Epoch 4720|Loss: 0.1794|Acc: 94.7069%|Acc test: 94.6604%\n",
      "Epoch 4730|Loss: 0.1792|Acc: 94.7140%|Acc test: 94.6604%\n",
      "Epoch 4740|Loss: 0.1791|Acc: 94.7140%|Acc test: 94.6621%\n",
      "Epoch 4750|Loss: 0.1789|Acc: 94.7140%|Acc test: 94.6675%\n",
      "Epoch 4760|Loss: 0.1788|Acc: 94.7282%|Acc test: 94.6675%\n",
      "Epoch 4770|Loss: 0.1786|Acc: 94.7282%|Acc test: 94.6710%\n",
      "Epoch 4780|Loss: 0.1785|Acc: 94.7425%|Acc test: 94.6781%\n",
      "Epoch 4790|Loss: 0.1783|Acc: 94.7425%|Acc test: 94.6870%\n",
      "Epoch 4800|Loss: 0.1782|Acc: 94.7425%|Acc test: 94.6906%\n",
      "Epoch 4810|Loss: 0.1780|Acc: 94.7496%|Acc test: 94.6906%\n",
      "Epoch 4820|Loss: 0.1779|Acc: 94.7496%|Acc test: 94.6977%\n",
      "Epoch 4830|Loss: 0.1777|Acc: 94.7638%|Acc test: 94.6977%\n",
      "Epoch 4840|Loss: 0.1776|Acc: 94.7638%|Acc test: 94.6995%\n",
      "Epoch 4850|Loss: 0.1774|Acc: 94.7709%|Acc test: 94.7048%\n",
      "Epoch 4860|Loss: 0.1773|Acc: 94.7709%|Acc test: 94.7102%\n",
      "Epoch 4870|Loss: 0.1772|Acc: 94.7709%|Acc test: 94.7173%\n",
      "Epoch 4880|Loss: 0.1770|Acc: 94.7780%|Acc test: 94.7208%\n",
      "Epoch 4890|Loss: 0.1769|Acc: 94.7851%|Acc test: 94.7262%\n",
      "Epoch 4900|Loss: 0.1767|Acc: 94.7923%|Acc test: 94.7315%\n",
      "Epoch 4910|Loss: 0.1766|Acc: 94.7923%|Acc test: 94.7368%\n",
      "Epoch 4920|Loss: 0.1764|Acc: 94.8065%|Acc test: 94.7333%\n",
      "Epoch 4930|Loss: 0.1763|Acc: 94.8136%|Acc test: 94.7404%\n",
      "Epoch 4940|Loss: 0.1762|Acc: 94.8136%|Acc test: 94.7422%\n",
      "Epoch 4950|Loss: 0.1760|Acc: 94.8207%|Acc test: 94.7493%\n",
      "Epoch 4960|Loss: 0.1759|Acc: 94.8207%|Acc test: 94.7511%\n",
      "Epoch 4970|Loss: 0.1757|Acc: 94.8349%|Acc test: 94.7528%\n",
      "Epoch 4980|Loss: 0.1756|Acc: 94.8349%|Acc test: 94.7546%\n",
      "Epoch 4990|Loss: 0.1755|Acc: 94.8349%|Acc test: 94.7546%\n"
     ]
    }
   ],
   "source": [
    "# Convert data to device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "# Create trainning loop\n",
    "torch.manual_seed(21)\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Trainning\n",
    "    y_logist = model(X_train)\n",
    "    y_pred = torch.softmax(y_logist,dim=1).argmax(dim=1)\n",
    "    # Calculate accuraccy\n",
    "    acc = torchmetrics.functional.accuracy(y_pred,y_train)\n",
    "    # Caculate loss\n",
    "    loss = loss_function(y_logist,y_train)\n",
    "    # Zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    # Update weight\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        y_logist_test = model(X_test)\n",
    "        y_pred_test = torch.softmax(y_logist_test,dim=1).argmax(dim=1)\n",
    "        acc_test = torchmetrics.functional.accuracy(y_pred_test,y_test)\n",
    "        print(f'Epoch {epoch}|Loss: {loss.item():.4f}|Acc: {acc.item() * 100:.4f}%|Acc test: {acc_test.item()*100:.4f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     23746\n",
      "           1       0.96      0.92      0.94     15193\n",
      "           2       0.95      0.98      0.97     17282\n",
      "\n",
      "    accuracy                           0.95     56221\n",
      "   macro avg       0.95      0.95      0.95     56221\n",
      "weighted avg       0.95      0.95      0.95     56221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "y_logist_test = model(X_test)\n",
    "y_pred_test = torch.softmax(y_logist_test,dim=1).argmax(dim=1)\n",
    "print(classification_report(y_test.cpu(),y_pred_test.cpu()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
